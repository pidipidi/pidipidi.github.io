<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daehyung&#39;s Homepage on Daehyung&#39;s Homepage</title>
    <link>https://pidipidi.github.io/index.xml</link>
    <description>Recent content in Daehyung&#39;s Homepage on Daehyung&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Daehyung Park</copyright>
    <lastBuildDate>Thu, 06 Apr 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dynamic Obstacle Avoidnace</title>
      <link>https://pidipidi.github.io/project/dmp/</link>
      <pubDate>Thu, 20 Apr 2017 22:46:22 -0400</pubDate>
      
      <guid>https://pidipidi.github.io/project/dmp/</guid>
      <description>&lt;p&gt;Robots in a human environment need to be compliant. This compliance requires that a pre-planed movement can be adapted to an obstacle that may be moving or appearing unexpectedly. Here, we present a general framework for movement generation and mid-flight adaptation to obstacles. For robust motion generation, Ijspeert et al developed the dynamic movement primitives, which represent a demonstrated movement with a differential equation. This equation allows adding a perturbing force without sacrificing stability. We extend this framework such that arbitrary movements in end-effector space can be represented - which was not possible before. Furthermore, we include obstacle avoidance by adding to the equation of motion a repellent force - a gradient of a potential field centered around the obstacle. In addition, this article studies the effect of different potential fields and shows how to avoid obstacle-link collisions within this framework. We demonstrate the abilities of our framework in simulations and with an anthropoid robot arm.&lt;/p&gt;

&lt;p&gt;Supervised Learning (Dynamic Movement Primitives)
Obstacle Avoidance by Potential Field Approach
7-DoF Redudant Manipulator
Null-Space Elbow Avoidance&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industrial Manipulation</title>
      <link>https://pidipidi.github.io/project/samsung/</link>
      <pubDate>Wed, 19 Apr 2017 23:52:00 -0400</pubDate>
      
      <guid>https://pidipidi.github.io/project/samsung/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Haptic manipulation</title>
      <link>https://pidipidi.github.io/project/haptic-manipulation/</link>
      <pubDate>Wed, 19 Apr 2017 23:19:50 -0400</pubDate>
      
      <guid>https://pidipidi.github.io/project/haptic-manipulation/</guid>
      <description>&lt;p&gt;We consider the problem of enabling a robot
to efficiently obtain a dense haptic map of its visible sur-
roundings using the complementary properties of vision and
tactile sensing. Our approach assumes that visible surfaces that
look similar to one another are likely to have similar haptic
properties. We present an iterative algorithm that enables a
robot to infer dense haptic labels across visible surfaces when
given a color-plus-depth (RGB-D) image along with a sequence
of sparse haptic labels representative of what could be obtained
via tactile sensing. Our method uses a color-based similarity
measure and connected components on color and depth data.
We evaluated our method using several publicly available RGB-
D image datasets with indoor cluttered scenes pertinent to robot
manipulation. We analyzed the effects of algorithm parameters
and environment variation, specifically the level of clutter and
the type of setting, like a shelf, table top, or sink area. In
these trials, the visible surface for each object consisted of an
average of 8602 pixels, and we provided the algorithm with
a sequence of haptically-labeled pixels up to a maximum of
40 times the number of objects in the image. On average, our
algorithm correctly assigned haptic labels to 76.02% of all of the
object pixels in the image given this full sequence of labels. We
also performed experiments with the humanoid robot DARCI
reaching in a cluttered foliage environment while using our
algorithm to create a haptic map. Doing so enabled the robot
to reach goal locations using a single plan after a single greedy
reach, while our previous tactile-only mapping method required
5 or more plans to reach each goal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multimodal Execution Monitor</title>
      <link>https://pidipidi.github.io/project/execution_monitor/</link>
      <pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pidipidi.github.io/project/execution_monitor/</guid>
      <description>&lt;p&gt;Online detection of anomalous execution can be valuable for robot manipulation, enabling robots to operate more safely, determine when a behavior is inappropriate, and otherwise exhibit more common sense. By using multiple complementary sensory modalities, robots could potentially detect a wider variety of anomalies, such as anomalous contact or a loud utterance by a human. However, task variability and the potential for false positives make online anomaly detection challenging, especially for long-duration manipulation behaviors. In this paper, we provide evidence for the value of multimodal execution monitoring and the use of a detection threshold that varies based on the progress of execution. Using a data-driven approach, we train an execution monitor that runs in parallel to a manipulation behavior. Like previous methods for anomaly detection, our method trains a hidden Markov model (HMM) using multimodal observations from non-anomalous executions. In contrast to prior work, our system also uses a detection threshold that changes based on the execution progress. We evaluated our approach with haptic, visual, auditory, and kinematic sensing during a variety of manipulation tasks performed by a PR2 robot. The tasks included pushing doors closed, operating switches, and assisting able-bodied participants with eating yogurt. In our evaluations, our anomaly detection method performed substantially better with multimodal monitoring than single modality monitoring. It also resulted in more desirable ROC curves when compared with other detection threshold methods from the literature, obtaining higher true positive rates for comparable false positive rates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robot-Assisted Feeding System</title>
      <link>https://pidipidi.github.io/project/assistive_feeding/</link>
      <pubDate>Mon, 17 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pidipidi.github.io/project/assistive_feeding/</guid>
      <description>&lt;p&gt;General-purpose mobile manipulators have the potential to serve as a versatile form of assistive technology. However, their complexity creates challenges, including the risk of being too difficult to use. We present a proof-of-concept robotic system for assistive feeding that consists of a Willow Garage PR2, a high-level web-based interface, and specialized autonomous behaviors for scooping and feeding feed. As a step towards use by people with disabilities, we evaluated our system with 5 able-bodied participants. All 5 successfully ate yogurt using the system and reported high rates of success for the systemâ€™s autonomous behaviors. Also, Henry Evans, a person with severe quadriplegia, operated the system and successfully fed himself. In general, people who operated the system reported that it was easy to use, including Henry. The feeding system also incorporates corrective actions designed to be triggered either autonomously or by the user.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Assistive Robotic System with a Robotic Bed and a Mobile Manipulator</title>
      <link>https://pidipidi.github.io/publication/iros_2017_collaboration/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pidipidi.github.io/publication/iros_2017_collaboration/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multimodal Anomaly Detection for Assistive Robots</title>
      <link>https://pidipidi.github.io/publication/auro_2017_ad/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pidipidi.github.io/publication/auro_2017_ad/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
